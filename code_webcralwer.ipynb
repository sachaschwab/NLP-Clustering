{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JCU MA5851\n",
    "<p style=\"line-height: 1.5; font-size:14pt\">\n",
    "    Student: Sacha Schwab <br>\n",
    "    Location: Zurich, Switzerland\n",
    "</p>\n",
    "\n",
    "\n",
    "# Assessment 3 - Code for Part Two (WebCrawling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statics\n",
    "dir_path = 'data/'\n",
    "raw_file_name = 'raw_data.csv'\n",
    "yahoo_url = \"https://finance.yahoo.com/cryptocurrencies/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yahoo_crypto_crawler_pipeline(file_path):\n",
    "    # Get new urls\n",
    "    df = get_yahoo_crypto_news_only_url(file_path)\n",
    "    df.to_csv(dir_path + raw_file_name, index = False)\n",
    "    # Get the content\n",
    "    df = crawl_new_articles(file_path)\n",
    "    df.to_csv(dir_path + raw_file_name, index = False)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST cell above (uncomment for testing)\n",
    "# df = yahoo_crypto_crawler_pipeline(dir_path + raw_file_name)\n",
    "#df = pd.read_csv(dir_path + raw_file_name)\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url):\n",
    "    ''' Request and retrieve html from a webpage, and status code\n",
    "        Input: the url to be crawled\n",
    "        Output: A timestamp and status code of the request and the page content\n",
    "        Prints: The url loaded at the moment, for monitoring purpose\n",
    "    '''\n",
    "    print(\"Getting url: \" + url)\n",
    "    status_codes = {}\n",
    "    page = requests.get(url)\n",
    "    status_code = page.status_code\n",
    "    timestamp = datetime.datetime.now()\n",
    "    return(status_code, page)\n",
    "\n",
    "def get_soup(page):\n",
    "    ''' Convert the page html content from a request into a beutifulsoup soup\n",
    "        Input: The page html content\n",
    "        Output: The soup\n",
    "    '''\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    ''' Extract the title from a Yahoo articles page\n",
    "        Input: Soup\n",
    "        Output: The title text\n",
    "    '''\n",
    "    # Extract the title\n",
    "    if soup.find('header', class_='caas-title-wrapper'):\n",
    "        title = soup.find('header', class_='caas-title-wrapper').text.strip()\n",
    "        return(title)\n",
    "    else:\n",
    "        return('')\n",
    "\n",
    "def get_date_time(soup):\n",
    "    ''' Extract the date stamp from a Yahoo articles page\n",
    "        Input: Soup\n",
    "        Output: The date text\n",
    "    '''\n",
    "    # Extract the date\n",
    "    if soup.find('div', class_='caas-attr-time-style'):\n",
    "        date = soup.find('div', class_='caas-attr-time-style').text.split(\"Â·\")[0]\n",
    "        return(date)\n",
    "    else:\n",
    "        return('')\n",
    "\n",
    "def get_text(soup):\n",
    "    ''' Extract the body articles text\n",
    "        Input: Soup\n",
    "        Output: The article body text\n",
    "    '''\n",
    "    # Extract the article text\n",
    "    art_text = soup.find('div', class_='caas-body').text\n",
    "    return(art_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_crypto_news_only_url(file_path):\n",
    "    ''' Extract the urls currently feature on Yahoo cryptocurrency news\n",
    "        Input: n/a\n",
    "        Output: Urls (i.e. new ones) extracted here are directly save\n",
    "                into the raw data file.\n",
    "    '''\n",
    "    # Get the soup and the status of the response\n",
    "    df = pd.read_csv(file_path)\n",
    "    status, page = get_page_content(yahoo_url)\n",
    "    soup = get_soup(page)\n",
    "    # Loop through html items and extract the data\n",
    "    titles_tags = soup.find_all(\"a\", class_=\"js-content-viewer\", href=True)\n",
    "    for title_tag in titles_tags:\n",
    "        url = 'https://yahoo.com' + title_tag['href']\n",
    "        # Proceed only if the url does not yet exist\n",
    "        if not (url in df['url']):\n",
    "            data = {}\n",
    "            data['url'] = url\n",
    "            data['title'] = ''\n",
    "            df = df.append(data, ignore_index=True)\n",
    "            print('Added: ' + url)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://yahoo.com/news/crypto-daily-movers-sha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://yahoo.com/video/day-giving-december-9-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://yahoo.com/news/jpmorgan-gave-away-nfts...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://yahoo.com/news/michael-schaiman-talks-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://yahoo.com/news/bubblehouse-partners-po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  \\\n",
       "378         378           NaN   \n",
       "379         379           NaN   \n",
       "380         380           NaN   \n",
       "381         381           NaN   \n",
       "382         382           NaN   \n",
       "\n",
       "                                                   url title text date_time  \n",
       "378  https://yahoo.com/news/crypto-daily-movers-sha...   NaN  NaN       NaN  \n",
       "379  https://yahoo.com/video/day-giving-december-9-...   NaN  NaN       NaN  \n",
       "380  https://yahoo.com/news/jpmorgan-gave-away-nfts...   NaN  NaN       NaN  \n",
       "381  https://yahoo.com/news/michael-schaiman-talks-...   NaN  NaN       NaN  \n",
       "382  https://yahoo.com/news/bubblehouse-partners-po...   NaN  NaN       NaN  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST the cell above (uncomment lines here)\n",
    "# df = get_yahoo_crypto_news_only_url(dir_path + raw_file_name)\n",
    "# df.tail()\n",
    "# df.to_csv(dir_path + 'mock_data.csv')\n",
    "# df = pd.read_csv(dir_path + 'mock_data.csv')\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_new_articles(file_path):\n",
    "    ''' Crawl Yahoo articles newly obtained\n",
    "        Input: Path to the file containing the new urls\n",
    "        Output: Dataframe with titles and body text data to each new url\n",
    "        Prints: The url crawled at the moment\n",
    "    '''\n",
    "    # Read the raw articles data\n",
    "    print('Opening raw data file')\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Backup just in case\n",
    "    df.to_csv(dir_path + 'raw_data_backup' + str(date.today()) + '.csv')\n",
    "    # GOVERNANCE: Clean backups from time to time\n",
    "\n",
    "    # Erase NaNs\n",
    "    df = df.fillna('')\n",
    "    # Filter the urls that have not yet been crawled\n",
    "    df_todo = df[df['text'] == '']\n",
    "\n",
    "    # Loop through urls to crawl and get the data\n",
    "    i = 0\n",
    "    for index, row in df_todo.iterrows():\n",
    "        # Print 'status'\n",
    "        print('Now crawling: ' + row['url'])\n",
    "        # Dict to hold the sample data\n",
    "        sample = {}\n",
    "        # Get response code\n",
    "        response_code, page = get_page_content(row['url'])\n",
    "        if response_code == 200:\n",
    "            # Get the soup\n",
    "            soup = get_soup(page)\n",
    "            title = get_title(soup)\n",
    "            if (len(title) > 0):\n",
    "                df.loc[index, 'title'] = title\n",
    "                text = get_text(soup)\n",
    "                if len(text) > 0:\n",
    "                    df.loc[index, 'text'] = text\n",
    "                    df.loc[index, 'date_time'] = get_date_time(soup)\n",
    "                    \n",
    "                else:\n",
    "                    print('dropping row')\n",
    "                    df = df.drop(index = index)\n",
    "            else:\n",
    "                df = df.drop(index = index)\n",
    "        else:\n",
    "            print('dropping')\n",
    "            df = df.drop(index = index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST the cell above (uncomment lines here)\n",
    "# df = crawl_new_articles(dir_path + 'mock_data.csv')\n",
    "# df.to_csv(dir_path + 'mock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yahoo_crypto_news():\n",
    "    # Yahoo url\n",
    "    yahoo_url = \"https://finance.yahoo.com/topic/crypto/\"\n",
    "    # The class for the titles we are interested in\n",
    "    title_class = 'mega-item-header-link'\n",
    "    # Yahoo url prefix\n",
    "    prefix = 'https://finance.yahoo.com/'\n",
    "    \n",
    "    # Get the soup and the status of the response\n",
    "    soup, status = get_page_content(yahoo_url)\n",
    "    \n",
    "    # Loop through html items and extract the data\n",
    "    article_items = soup.find_all('li', class_='js-stream-content Pos(r)')\n",
    "    if article_items:\n",
    "        i = 0\n",
    "        for item in article_items:\n",
    "            sample = {}\n",
    "            item_title = item.find(\"a\", class_=soup_class)\n",
    "            if item_title:\n",
    "                sample['title'] = item_title.text.strip()\n",
    "            a_class = item.find(\"a\", class_=\"js-content-viewer\", href=True)\n",
    "            if a_class:\n",
    "                url = prefix + a_class['href']\n",
    "                if url:\n",
    "                    sample['url'] = url\n",
    "                    # Get the full article text from url\n",
    "                    art_soup, art_status = get_page_content(url)\n",
    "                    art_text = ''\n",
    "                    for p in art_soup.find_all('p'):\n",
    "                        art_text = art_text + p.text\n",
    "                    #print(art_text)\n",
    "                    sample['text'] = art_text\n",
    "            print(sample)\n",
    "            if (i == 0):\n",
    "                break\n",
    "        else:\n",
    "            print('None')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cryptocurrencies list crawler (under construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://finance.yahoo.com/cryptocurrencies/?count=25&offset=375'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = ''\n",
    "page = requests.get(url, timeout=1) \n",
    "soup = get_soup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC-USD\n",
      "Bitcoin USD\n",
      "ETH-USD\n",
      "Ethereum USD\n",
      "BNB-USD\n",
      "BinanceCoin USD\n",
      "USDT-USD\n",
      "Tether USD\n",
      "SOL1-USD\n",
      "Solana USD\n",
      "ADA-USD\n",
      "Cardano USD\n",
      "USDC-USD\n",
      "USDCoin USD\n",
      "XRP-USD\n",
      "XRP USD\n",
      "HEX-USD\n",
      "HEX USD\n",
      "DOT1-USD\n",
      "Polkadot USD\n",
      "AVAX-USD\n",
      "Avalanche USD\n",
      "DOGE-USD\n",
      "Dogecoin USD\n",
      "LUNA1-USD\n",
      "Terra USD\n",
      "SHIB-USD\n",
      "SHIBA INU USD\n",
      "CRO-USD\n",
      "CryptocomCoin USD\n",
      "MATIC-USD\n",
      "MaticNetwork USD\n",
      "LTC-USD\n",
      "Litecoin USD\n",
      "UNI3-USD\n",
      "Uniswap USD\n",
      "ALGO-USD\n",
      "Algorand USD\n",
      "LINK-USD\n",
      "Chainlink USD\n",
      "BCH-USD\n",
      "BitcoinCash USD\n",
      "TRX-USD\n",
      "TRON USD\n",
      "AXS-USD\n",
      "AxieInfinity USD\n",
      "XLM-USD\n",
      "Stellar USD\n",
      "DAI1-USD\n",
      "Dai USD\n"
     ]
    }
   ],
   "source": [
    "for s in soup.find_all(attrs={\"aria-label\" : \"Symbol\"}):\n",
    "    print(s.text)\n",
    "    print(s.find_next_sibling().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-166-4101fb83015d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-166-4101fb83015d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    WebDriver driver => new FirefoxDriver()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "WebDriver driver => new FirefoxDriver()\n",
    "driver.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS)\n",
    "driver.get(url)\n",
    "html = browser.page_source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
