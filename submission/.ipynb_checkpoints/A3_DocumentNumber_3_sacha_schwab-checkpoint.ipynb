{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkblue'>MA5851 A3 Assessment Report Part Three (NLP Tasks)</font>\n",
    "\n",
    "<font color='darkblue'>Student: </font>Sacha Schwab\n",
    "\n",
    "<font color='darkblue'>Location: </font>Zurich, Switzerland\n",
    "\n",
    "<font color='darkblue'>Date: </font>6 December 2021\n",
    "\n",
    "### Note: \n",
    "Figures (images) are not correctly loaded by github file; please use the documents submitted on JCU Learn in the assessment submission folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    1. Preliminary thoughts\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\">\n",
    "    Preliminary thoughts\n",
    "</h2>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    <p style=\"line-height: 1.5; font-size:12pt\">\n",
    "Basic task: Group articles into events they report about.\n",
    "</p>\n",
    "    <ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "        <li>Extract keyword and named enttiies features (Li et al., 2021, p.1).</li>\n",
    "        <li>Clustering</li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "Challenges:\n",
    "</p>\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Titles often do not fully represent the event</li>\n",
    "      <li>Side events in same article</li>\n",
    "      <li>Different classes of articles e.g. regulatory, acquisitions etc.\n",
    "      <li>Verb-argument patterns expected to be not applicable (Rusu et al., 2014)\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    2. Literature Review\n",
    "</h1>\n",
    "<style>\n",
    "  p {color:blue;}\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Numerous important events happen everyday and everywhere. The sources documenting events are numerous and diverse, with different narrative styles (Xiang and Wang, 2019). The literature is diverse when it comes to event extraction. Possible reasons for this diversity can be identified:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li>Coffee</li>\n",
    "  <li>Tea</li>\n",
    "  <li>Milk</li>\n",
    "</ul>\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "The literature pertaining to event clustering is diverse insofar as it reflects a variety of\n",
    "</p>\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>definitions of \"event\", e.g. \"Actor-Action-Object-Time\" (e.g. in Xing et al., 2018), or a \"who, when, where, what, why, how\" syntax (see Xiang and Wang, 2019), or syntax schemes according to the event category under investigation (e.g. \"attacker-target-instrument-time-place\", see Li et al., 2021, p.2)</li>\n",
    "    <li>'event extraction' emerged to a separate study field with a diverse range of approaches (see below; overview see see Xing et al., 2018).;</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "Li et al. (2021) investigated the approaches applied by the literature, and divided them into 3 groups, which are: (1) pattern-matching, (2) machine learning, and (3) deep learning. They also state that the recent work focuses on combinations of deep learning techniques.<br><br>\n",
    "As to clustering, Goya et al. (2018, p. 24) find that a combination of named entities and keywords improves the clustering quality. In particular, named entity recognition has shown a remarkable improvement for clustering.<br><br>\n",
    "Capdevila et al. (2016, p. 1 f) and further resources describe DBSCAN constitute a well-known approach to event detection due its noise resilience capability.<br><br>\n",
    "Cao et al., 2012 conclude from their research that a weighted combination of named entities and keywords are significant to clustering quality.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    3. Approach\n",
    "</h1>\n",
    "\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Identify named entities and their importance;\n",
    "  <li>Identify keywords using word embedding and taking the words with x top weights;\n",
    "  <li>Sentiment analysis;\n",
    "  <li>Normalise numerical features from the 3 steps above (i.e. entities weights, keyword weigths, sentiment scores)\n",
    "  <li>Apply clustering technique;\n",
    "  <li>Measure the performance of the model variations.\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Algorithms applied:\n",
    "</p>\n",
    "\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Named entity extraction and relations: NLTK\n",
    "  <li>Embeddings: TF-IDF\n",
    "  <li>For clustering: DBSCAN (see indications in the literature).\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    5. Data Wrangling\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    In this part the harvested data generated by the web crawler is preprocessed and tokenised for further use for NLP tasks.<br>\n",
    "    Data preprecessing follows a rather standard approach as per literature indications, learning materials in MA5851, and online tutorials. It includes lower-casing, erasure of one-character words and symbols as well lemmatization. The latter is applied knowing that it is slower, however appears to make more sense since it better reflects natural speech than stemming (Jabeen, 2018).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and visuallisation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">Reference is made to section (k) of the submitted report for part 2 of this assessment.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    6. NLP Tasks\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding / Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Since TF-IDF computes the importance of a term in a document (taking all other documents in the corpus into account), it appears appropriate for finding keywords (see e.g. Ellis, 2019).<br>\n",
    "    The TfidfVectorizer from NLTK package was employed for the purpose of producing TF-IDF vectors.<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "   For each document \n",
    "    - every word in the tokenised text is looked up in the TF-IDF matrix, \n",
    "    - its weight collected and put into a dataframe for sorting, \n",
    "    - evaluate the weight value of the (top) x'st word, and \n",
    "    - drop all other words. \n",
    "The two arrays (top x words and their weights) are returned for their use in the clustering task below.<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "   For this rather complex task a number of pre-trained models can be employed. The Spacy framework was applied with satisfactory results. The \"select top x\" approach as per above was employed.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "   For this NLP task a rather simplistic approach was selected for efficiency reasons, i.e. application of the VADER sentiment analysis (SentimentIntensityAnalyzer from NLTK), which was run for every document.<br>\n",
    "   The resulting compound score is the sum of positive, negative and neutral scores, adn these are normalized between -1 (highly  negative) and +1 (highly positive).<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Task Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output from keyword extraction\n",
    "\n",
    "<img src=\"assets/keyword_freq.png\" alt=\"HDBSCAN output\" width=\"1000\" align=\"center\"></img>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Frequency distribution of extracted entities</p><br>\n",
    "\n",
    "<img src=\"assets/keyword_weight_hist.png\" alt=\"HDBSCAN output\" width=\"500\" align=\"center\"></img>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Frequency distribution of extracted entities</p><br>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\"><br>More work needs to be done:\n",
    "    (a) non-english stopwords (\"el\", \"de\" etc.) and \n",
    "    (b) frequent unwanted concatenations happened</p>\n",
    "\n",
    "#### Output from entity extraction\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\"><br>Also here the output could be more meaningful. The distribution of weights indicates that the entities' importance is generally rather inferior, i.e. the entities, as to be observed above, are not represented among the keywords as it might be expected. </p>\n",
    "\n",
    "<img src=\"assets/entities_distribution.png\" alt=\"HDBSCAN output\" width=\"1000\" align=\"center\"></img>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Frequency distribution of extracted entities</p><br>\n",
    "<img src=\"assets/entities_weights.png\" alt=\"HDBSCAN output\" width=\"800\"/></img>\n",
    "\n",
    "\n",
    "#### Output from sentimeent analysis\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Provided sentiment scores are unequally distributed: The sentiment representations of titles and body texts do not match at all.\n",
    "</p>\n",
    "<img src=\"assets/sent_distribution.png\" alt=\"HDBSCAN output\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    7. Machine Learning\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Variations of eps and the min_samples parameters were run, with the normalised 10 top keywords and entities, and the sentiment scores served as input.<br>\n",
    "Result: DBSCAN would not present meaningful results: The algorithm would recognise either a cluster label for every article, or find only noise and one label (0). <br></p>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">Conclusion: DBSCAN cannot be employed for the task at hand. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Parameter variations for HDBSCAN provide results that are more in the space of expectations (see above considerations), i.e. a rather high level of noise and a rather small number of actual clusters as shown in the visualisation below.<br>\n",
    "Based on preliminary checks, variations were applied to parameters leaf_size (20 and 80) and min_samples (none and 1).  </p><br>\n",
    "<img src=\"assets/hdbscan_output.png\" alt=\"HDBSCAN output\" width=\"1000\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Resulting cluster distribution from HDBSCAN</p>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">Also, the silhouette scores are not in a satisfactory range:</p>\n",
    "<img src=\"assets/silh_hdbscan.png\" alt=\"HDBSCAN Silhouette scores\" width=\"700\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Silhouette scores obtained</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions (\"Show case\")\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "The following shows the result for a sample taken from the above used data themselves. It is not very promising (only 2 of 4 results match), however a valid start for further iterations.</p>\n",
    "\n",
    "<img src=\"assets/cluster_display_.png\" alt=\"HDBSCAN Silhouette scores\" width=\"700\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: HDBSCAN cluster match result</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    Conclusion\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    More work needs to be invested into extraction and handling of features. For clustering, an alternative approach needs to be considered.<br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    Code\n",
    "</h1>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">The code for this part of the assessment is available in the seperate file \"A3_DocumentNumber_2_Code_sacha_schwab.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count for all documents in this assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count in document 1: 1153\n",
      "Count in document 2: 1272\n",
      "Count in document 3: 1382\n",
      "Total words in all documents: 3807\n"
     ]
    }
   ],
   "source": [
    "# Finding the number of words in markdown fields\n",
    "import io\n",
    "from nbformat import current\n",
    "nbs = []\n",
    "with io.open('A3_DocumentNumber_1_sacha_schwab.ipynb', 'r', encoding='utf-8') as f:\n",
    "    nbs.append(current.read(f, 'json'))\n",
    "with io.open('A3_DocumentNumber_2_sacha_schwab.ipynb', 'r', encoding='utf-8') as f:\n",
    "    nbs.append(current.read(f, 'json'))\n",
    "with io.open('A3_DocumentNumber_3_sacha_schwab.ipynb', 'r', encoding='utf-8') as f:\n",
    "    nbs.append(current.read(f, 'json'))\n",
    "total_count = 0\n",
    "i = 1\n",
    "for nb in nbs:\n",
    "    word_count = 0\n",
    "    for cell in nb.worksheets[0].cells:\n",
    "        if cell.cell_type == \"markdown\":\n",
    "            word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "    print('Count in document ' + str(i) + ': ' + str(word_count))\n",
    "    total_count = total_count + word_count\n",
    "    i = i + 1\n",
    "print('Total words in all documents: ' + str(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
