{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkblue'>MA5851 A3 Assessment Report Part Two (WebCrawler)</font>\n",
    "\n",
    "<font color='darkblue'>Student: </font>Sacha Schwab\n",
    "\n",
    "<font color='darkblue'>Location: </font>Zurich, Switzerland\n",
    "\n",
    "<font color='darkblue'>Date: </font>3 December 2021\n",
    "\n",
    "Github link to repository: https://github.com/sachaschwab/NLP-Clustering\n",
    "\n",
    "### Note: \n",
    "Figures (images) are not correctly loaded by github file; please use the documents submitted on JCU Learn in the assessment submission folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    a) Websites consumed\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yahoo Finance (crawled with BeautifulSoup)\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Yahoo Finance collects news articles from multiple sources such as Bloomberg and Reuters. The advantage is that the articles are presented in Yahoo's html format. Also, it features a \"Cryptocurrency\" news section. Therefore, and since I have not found or encountered any limitations to webcrawling, this resource appears appropriate for the task at hand.<br>\n",
    "<br>URLs:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Main site: https://finance.yahoo.com/topic/crypto/</li>\n",
    "  <li>Individual articles may have different prefix urls, however the crawler is flexible in that regard, since the individual article URLs are extracted from the main site.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yahoo Cryptocurrencies (crawled with Selenium)\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    As to be seen in the NLP part of this project, a list of current cryptocurrencies is required for injection into e.g. named entitiy extraction. Since the names are part of a table which takes on 25 items, the user needs to click through all the tables to get the names of all cryptocurrencies.<br>\n",
    "    Therefore, Selenium was used to click the button element, getting the next 25 items, and so on, until all approx. 375 names are retrieved.\n",
    "<br><br>URL: https://finance.yahoo.com/cryptocurrencies\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    b) Rationale for extraction\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    The aim of this project requires not only the gathering of high-quality news text, but also to achieve a corpus of the size indicated in the assessment outline, i.e. a minimum of 100-300 documents, since the texts appear medium sized. Further, it is beneficial to collect article texts from various domains so to avoid uniformity of the format i.e. to produce a model that covers a wider range of text structure and lengths. Yahoo Finance ticks all these requirements and therefore appears to be a valid choice.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    c) Content coverage\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    A preliminary manual review of Yahoo Finance (Cryptocurrencies) revealed that the number and range of articles appears interesting, since it features not only brief market event comments (such as with Bloomberg) but also developing stories. This covers the purpose of this project, i.e. to provide per-event-clustering of news articles.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    d) Complexity of the content layout\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    The html content layout is not highly complex, however requires some html skills to de-code it for the purpose of webcrawling, in particular since the pages are rendered by react-js engine. Therefore, the tag classes are presented can be quite tricky when it comes to interpreting which substring actually triggers the tag, such as in 'caas-xray-wrapper caas-xray-wrapper-type-cards caas-xray-wrapper-position-top'. However, in the end the tags turn out to be quite straight forward.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:darkblue\">\n",
    "    e) Website/data copyright considerations\n",
    "</h3>\n",
    "<br>\n",
    "<b style=\"line-height: 1.5; font-size:12pt\"> Permitted guidelines check: </b><br>\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Public data only: Yahoo.com is public. There is no walled garden, and neither is it a paid service.</li>\n",
    "  <li>Previously allowed: A large number of resources was found on scraping Yahoo.com content; it therefore appears that Yahoo implicitly allows webcrawling.</li>\n",
    "  <li>Non-copyright-protected content: The content under the above mentioned URL does not contain any copyright protection notice and, at the time of issuing this report, it was not found under the CloudFlare protected website search. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    f) Metadata supplementation\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Supplementation of the articles' dates turned out to be sufficient, and the author does not appear relevant for the purpose of this project.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    g) Content extractor / WebCrawler workflow\n",
    "</h3>\n",
    "\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>The <b>technology component</b> used for the web crawler is limited to Beautifulsoup, which appeeared sufficient for the purpose of this project. There was no need to employ Selenium (e.g. to render older articles to the html content) since, to keep the data and the model up to date,  a daily run of the crawler is anyway necessary.</li>\n",
    "  <li>As outlined above, the <b>ccmplexity of the domains</b> is rather low. The  <b>targeted data</b> resides in the article pages, URLs of which were obtained by extracting the hrefs from the main page.</li>\n",
    "  <li>Some <b>sequencing</b> was applied by first processing the URLs of the new articles in the raw data frame csv, with subsequent crawling of the individual articles pages in a loop.\n",
    "  <li><b>Data storage</b> is achieved using csv format, which in light of the limited size of the data and for performance considerations appears appropriate.</li>\n",
    "</ul><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Basic workflow:\n",
    "</p>\n",
    "<img src=\"assets/webcrawler_workflow.png\" alt=\"workflow\" width=\"600\">![](assets/webcrawler_workflow.png)</img>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Webcrawler workflow</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    h) Python coding\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Python code as per code files (see links below) use PEP8 and PEP256 code format.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo Finance (articles)\n",
    "<h3 style=\"color:darkblue\">\n",
    "    i) Demonstration of the application of the WebCrawler\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Yahoo main page:</p>\n",
    "<img src=\"assets/main_page_screenshot.png\" alt=\"main page\" width=\"600\"\"/>\n",
    "\n",
    "                                                                       \n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Yahoo main page with inspection of the title</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    An article page sample:</p><br>\n",
    "<img src=\"assets/article_page_screenshot.png\" alt=\"article page\" width=\"600\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Yahoo individual article page with inspection of the title</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Daily webcrawling gif:</p><br>\n",
    "<img src=\"https://user-images.githubusercontent.com/10763939/144720471-b0f3648d-369d-4f06-957a-92880e5cffed.gif\" alt=\"daily crawler gif\" width=\"600\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Running webcrawler (gif)</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo Cryptocurrencies list\n",
    "\n",
    "<img src=\"assets/yahoo_cryptos_inspection.png\" alt=\"crypto inspection\" width=\"600\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Yahoo Cryptocurrencies list - Inspection of button element to repeatedly click</p><br><br>\n",
    "\n",
    "<img src=\"assets/selenium_crawling.gif\" alt=\"crypto inspection\" width=\"600\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Gif of activity log while crawling cryptocurency names with Selenium</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    j) Methodology of processing, cleaning, and storing harvested data for NLP tasking\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    The raw text data is stored as csv file. For preparation of the NLP tasks, removal of symbols, stop words performed, and the text is lemmatizsed usinng the NLTK WordLemmatizer. Also, lower casing is applied. Reference is made to document 3 (NLP tasks report). This procedure appears appropriate in light of the many resources recommending these steps in particular for TF-IDF vectorisation. However, resource would usually apply stemming instead of lemmatisation, however the latter is a personal preference since the output makes more sense.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    k) Summary and visualisation of the harvested data \n",
    "</h3>\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    The following summary reflects the status as per 5 December 2021</p>\n",
    "    \n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Number of documents: 385</li>\n",
    "  <li>Date range: From 2021-11-22 to: 2021-12-04</li>\n",
    "  <li>Mean word length of article body texts: 286.32987012987013 </li>\n",
    "  <li>Total corpus size: 822565 </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Frequency distribution of words in titles and body texts:</p>\n",
    "<img src=\"assets/text_distr.png\" alt=\"article page\" width=\"1000\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Distribution of word count in titles</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/title_distr.png\" alt=\"article page\" width=\"1000\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Distribution of word count in body texts</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/word_weights_distr.png\" alt=\"article page\" width=\"900\"/>\n",
    "<p style=\"font-size:10pt; text-align:center\">Figure: Distribution of weights in corpus</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Conclusion:</b><br>\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>The disbribution of corpus words follows Zipf's law</li>\n",
    "  <li>Considering the mean word lengths of documents, the number of documents (as per indication in the assessment outline) appears sufficient./li>\n",
    "  <li>Distribution of word count per documents appears within the expected range since the articles have different sources and represent different news types (daily summaries, brief summaries of events, extensive articles.</li>\n",
    "  <li>For the NLP tasks, this means that<br>\n",
    "        - seen compliance with Zipf's law, the corpus is 'ok to go' <br>\n",
    "        - high variance in text types and lenghts is a challenge, which I approach by selecting only x keywords (not the full TF-IDF vectors) and y entities (once extracted) <br>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:darkblue\">\n",
    "    Code\n",
    "</h1>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">The code for this part of the assessment is available in the seperate file \"A3_DocumentNumber_2_Code_sacha_schwab.ipynb\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
