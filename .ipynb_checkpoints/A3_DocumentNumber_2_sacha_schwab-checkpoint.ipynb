{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkblue'>MA5851 A3 Assessment Report Part Two (WebCrawler)</font>\n",
    "\n",
    "**<font color='darkblue'>Student: </font>Sacha Schwab**\n",
    "\n",
    "<font color='darkblue'>Location: </font>Zurich, Switzerland\n",
    "\n",
    "<font color='darkblue'>Date: </font>3 December 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    a) Websites consumed\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Yahoo Finance collects news articles from multiple sources such as Bloomberg and Reuters. The advantage is that the articles are presented in Yahoo's html format. Also, it features a \"Cryptocurrency\" news section. Therefore, and since I have not found or encountered any limitations to webcrawling, this resource appears appropriate for the task at hand.<br>\n",
    "<br>URLs:\n",
    "</p>\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Main site: https://finance.yahoo.com/topic/crypto/</li>\n",
    "  <li>Individual articles may have different prefix urls, however the crawler is flexible in that regard, since the individual article URLs are extracted from the main site.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    b) Rationale for extraction\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    The aim of this project requires not only the gathering of high-quality news text, but also to achieve a corpus of the size indicated in the assessment outline, i.e. a minimum of 100-300 documents, since the texts appear medium sized. Further, it is beneficial to collect article texts from various domains so to avoid uniformity of the format i.e. to produce a model that covers a wider range of text structure and lengths. Yahoo Finance ticks all these requirements and therefore appears to be a valid choice.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    c) Content coverage\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    A preliminary manual review of Yahoo Finance (Cryptocurrencies) revealed that the number and range of articles appears interesting, since it features not only brief market event comments (such as with Bloomberg) but also developing stories. This covers the purpose of this project, i.e. to provide per-event-clustering of news articles.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    d) Complexity of the content layout\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    The html content layout is not highly complex, however requires some html skills to de-code it for the purpose of webcrawling, in particular since the pages are rendered by react-js engine. Therefore, the tag classes are presented can be quite tricky when it comes to interpreting which substring actually triggers the tag, such as in 'caas-xray-wrapper caas-xray-wrapper-type-cards caas-xray-wrapper-position-top'. However, in the end the tags turn out to be quite straight forward.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:darkblue\">\n",
    "    e) Website/data copyright considerations\n",
    "</h3>\n",
    "<br>\n",
    "<b style=\"line-height: 1.5; font-size:12pt\"> Permitted guidelines check: </b><br>\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Public data only: Yahoo.com is public. There is no walled garden, and neither is it a paid service.</li>\n",
    "  <li>Previously allowed: A large number of resources was found on scraping Yahoo.com content; it therefore appears that Yahoo implicitly allows webcrawling.</li>\n",
    "  <li>Non-copyright-protected content: The content under the above mentioned URL does not contain any copyright protection notice and, at the time of issuing this report, it was not found under the CloudFlare protected website search. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    f) Metadata supplementation\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Supplementation of the articles' dates turned out to be sufficient, and the author does not appear relevant for the purpose of this project.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    g) Content extractor / WebCrawler workflow\n",
    "</h3>\n",
    "\n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>The <b>technology component</b> used for the web crawler is limited to Beautifulsoup, which appeeared sufficient for the purpose of this project. There was no need to employ Selenium (e.g. to render older articles to the html content) since, to keep the data and the model up to date,  a daily run of the crawler is anyway necessary.</li>\n",
    "  <li>As outlined above, the <b>ccmplexity of the domains</b> is rather low. The  <b>targeted data</b> resides in the article pages, URLs of which were obtained by extracting the hrefs from the main page.</li>\n",
    "  <li>Some <b>sequencing</b> was applied by first processing the URLs of the new articles in the raw data frame csv, with subsequent crawling of the individual articles pages in a loop.\n",
    "  <li><b>Data storage</b> is achieved using csv format, which in light of the limited size of the data and for performance considerations appears appropriate.</li>\n",
    "</ul><br>\n",
    "\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Basic workflow:\n",
    "</p>\n",
    "<img src=\"assets/webcrawler_workflow.png\" alt=\"workflow\" width=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    h) Python coding\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Python code as per code files (see links below) use PEP8 and PEP256 code format.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    i) Demonstration of the application of the WebCrawler\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Yahoo main page:</p>\n",
    "<img src=\"assets/main_page_screenshot.png\" alt=\"main page\" width=\"600\"\"/><br><br>\n",
    "    \n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    An article page sample:</p><br>\n",
    "<img src=\"assets/article_page_screenshot.png\" alt=\"article page\" width=\"600\"/><br><br>    \n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    Daily webcrawling gif:</p><br>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10763939/144720471-b0f3648d-369d-4f06-957a-92880e5cffed.gif\" alt=\"daily crawler gif\" width=\"600\"/><br><br>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    j) Methodology of processing, cleaning, and storing harvested data for NLP tasking\n",
    "</h3>\n",
    "\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    The raw text data is stored as csv file. For preparation of the NLP tasks, removal of symbols, stop words performed, and the text is lemmatizsed usinng the NLTK WordLemmatizer. Also, lower casing is applied. Reference is made to document 3 (NLP tasks report). This procedure appears appropriate in light of the many resources recommending these steps in particular for TF-IDF vectorisation. However, resource would usually apply stemming instead of lemmatisation, however the latter is a personal preference since the output makes more sense.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:darkblue\">\n",
    "    k) Summary and visualisation of the harvested data \n",
    "</h3>\n",
    "<p style=\"line-height: 1.5; font-size:12pt\">\n",
    "    This summary reflects the status as per 5 December 2021</p>\n",
    "    \n",
    "<ul style=\"line-height: 1.5; font-size:12pt\">\n",
    "  <li>Number of articles (i.e. corpus documents): 394</li>\n",
    "  <li>asdf</li>\n",
    "  <li>asdf </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
